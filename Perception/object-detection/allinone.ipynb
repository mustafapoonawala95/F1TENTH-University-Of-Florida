{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n# !pip install -U numpy\n# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version 1.9 --apt-packages libomp5 libopenblas-dev\n","metadata":{"execution":{"iopub.status.busy":"2021-08-30T16:43:47.940666Z","iopub.execute_input":"2021-08-30T16:43:47.940989Z","iopub.status.idle":"2021-08-30T16:43:47.94524Z","shell.execute_reply.started":"2021-08-30T16:43:47.940894Z","shell.execute_reply":"2021-08-30T16:43:47.944334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# !export XLA_USE_BF16=1\n# import torch_xla.core.xla_model as xm\n# print(xm.xla_device())","metadata":{"execution":{"iopub.status.busy":"2021-08-30T16:43:47.949449Z","iopub.execute_input":"2021-08-30T16:43:47.949692Z","iopub.status.idle":"2021-08-30T16:43:47.957289Z","shell.execute_reply.started":"2021-08-30T16:43:47.94967Z","shell.execute_reply":"2021-08-30T16:43:47.95635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Architecture\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass architecture(nn.Module):\n    def __init__(self, S, B, C):\n        # S= No.of boxes, B=No.of prediction boxes, C=No.of classes\n        super(architecture, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3,64,kernel_size=7,stride=2,padding=3),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.1),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(64,192,kernel_size=3,stride=1,padding=1),\n            nn.BatchNorm2d(192),\n            nn.LeakyReLU(0.1),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(192,128,kernel_size=1,stride=1,padding=0),\n            nn.Conv2d(128,256,kernel_size=3,stride=1,padding=1),\n            nn.Conv2d(256,256,kernel_size=1,stride=1,padding=0),\n            nn.Conv2d(256,512,kernel_size=3,stride=1,padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.1),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(512,256,kernel_size=1,stride=1,padding=0),\n            nn.Conv2d(256,512,kernel_size=3,stride=1,padding=1),\n            nn.Conv2d(512,256,kernel_size=1,stride=1,padding=0),\n            nn.Conv2d(256,512,kernel_size=3,stride=1,padding=1),\n            nn.Conv2d(512,256,kernel_size=1,stride=1,padding=0),\n            nn.Conv2d(256,512,kernel_size=3,stride=1,padding=1),\n            nn.Conv2d(512,256,kernel_size=1,stride=1,padding=0),\n            nn.Conv2d(256,512,kernel_size=3,stride=1,padding=1),\n            nn.Conv2d(512,512,kernel_size=1,stride=1,padding=0),\n            nn.Conv2d(512,1024,kernel_size=3,stride=1,padding=1),\n            nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.1),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=0),\n            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=0),\n            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(1024, 1024, kernel_size=3, stride=2, padding=1),\n            nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1),\n            \n            nn.Flatten(),\n            nn.Linear(1024 * S * S, 4096),\n            nn.Dropout(0.0),\n            nn.LeakyReLU(0.1),\n            nn.Linear(4096, S * S * (C + B * 5))\n        )\n        \n    def forward(self, x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T16:43:47.958776Z","iopub.execute_input":"2021-08-30T16:43:47.959201Z","iopub.status.idle":"2021-08-30T16:43:49.2298Z","shell.execute_reply.started":"2021-08-30T16:43:47.959165Z","shell.execute_reply":"2021-08-30T16:43:49.228998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## getDataset\nfrom PIL import Image\nimport torch\nimport pandas as pd\nimport os\n\nclass load_ds(torch.utils.data.Dataset):\n    def __init__(self, csv, images_dir, label_dir, S, B, C, transform=None):\n        self.associations = pd.read_csv(csv)\n        self.img_dir      = images_dir\n        self.lbl_dir      = label_dir\n        self.transform    = transform\n        self.S            = S\n        self.B            = B\n        self.C            = C\n        \n    def __len__(self):\n        return len(self.associations)\n        \n    def __getitem__(self, index):\n        img_path   = os.path.join(self.img_dir, self.associations.iloc[index, 0])\n        label_path = os.path.join(self.lbl_dir, self.associations.iloc[index, 1])\n        bboxes     = []\n        \n        with open(label_path) as x:\n            for i in x.readlines():\n                class_label, x, y, w, h = [float(x) if float(x) != int(float(x)) else int(x)\n                                            for x in i.replace(\"\\n\", \"\").split()]\n                \n                bboxes.append([class_label, x, y, w, h])\n        \n        img = Image.open(img_path)\n        if self.transform:\n            for i in self.transform:\n                img, bboxes = i(img), bboxes\n                \n        \n        labels = torch.zeros((self.S, self.S, self.C + self.B*5))\n        for box in bboxes:\n            class_label, x, y, w, h = box\n            class_label = int(class_label)\n\n            # i,j represents the cell row and cell column\n            i, j = int(self.S * y), int(self.S * x)\n            x_cell, y_cell = self.S * x - j, self.S * y - i\n            w_cell, h_cell = (w*self.S, h*self.S)\n            \n            \n            if labels[i, j, 20] == 0:\n                # Set 1 if there exists an object\n                labels[i, j, 20] = 1\n\n                # Box coordinates\n                box_coords = torch.tensor([x_cell, y_cell, w_cell, h_cell])\n                labels[i, j, 21:25] = box_coords\n\n                # one hot encoding for class label\n                labels[i, j, class_label] = 1\n\n        return img, labels","metadata":{"execution":{"iopub.status.busy":"2021-08-30T16:43:49.231488Z","iopub.execute_input":"2021-08-30T16:43:49.23179Z","iopub.status.idle":"2021-08-30T16:43:49.246072Z","shell.execute_reply.started":"2021-08-30T16:43:49.231761Z","shell.execute_reply":"2021-08-30T16:43:49.245181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom collections import Counter\n\ndef iou(x, y, box_format):\n\n        if box_format == \"box\":\n            box1_x1 = x[..., 0]\n            box1_y1 = x[..., 1]\n            box1_x2 = x[..., 2]\n            box1_y2 = x[..., 3]\n            box2_x1 = y[..., 0]\n            box2_y1 = y[..., 1]\n            box2_x1 = y[..., 2]\n            box2_y1 = y[..., 3]\n        elif box_format == \"midpoint\":\n            box1_x1 = x[..., 0:1] - x[..., 2:3] / 2\n            box1_y1 = x[..., 1:2] - x[..., 3:4] / 2\n            box1_x2 = x[..., 0:1] + x[..., 2:3] / 2\n            box1_y2 = x[..., 1:2] + x[..., 3:4] / 2\n            box2_x1 = y[..., 0:1] - y[..., 2:3] / 2\n            box2_y1 = y[..., 1:2] - y[..., 3:4] / 2\n            box2_x2 = y[..., 0:1] + y[..., 2:3] / 2\n            box2_y2 = y[..., 1:2] + y[..., 3:4] / 2\n\n\n        x1 = torch.max(box1_x1, box2_x1)\n        y1 = torch.max(box1_y1, box2_y1)\n        x2 = torch.min(box1_x2, box2_x2)\n        y2 = torch.min(box1_y2, box2_y2)\n\n\n        # .clamp(0) is for the case when they do not intersect\n        intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n\n        box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n        box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n\n        return intersection / (box1_area + box2_area - intersection + 1e-6)\n    \ndef nms(bboxes, iou_threshold, threshold, box_format=\"box\"):\n\n    assert type(bboxes) == list\n\n    bboxes = [box for box in bboxes if box[1] > threshold]\n    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n    bboxes_after_nms = []\n\n    while bboxes:\n        chosen_box = bboxes.pop(0)\n\n        bboxes = [\n            box\n            for box in bboxes\n            if box[0] != chosen_box[0]\n            or iou(torch.tensor(chosen_box[2:]),\n                    torch.tensor(box[2:]),\n                    box_format=box_format)\n            < iou_threshold\n        ]\n\n        bboxes_after_nms.append(chosen_box)\n\n    return bboxes_after_nms\n\n\ndef get_map(\n    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n):\n\n    # list storing all AP for respective classes\n    average_precisions = []\n\n    # used for numerical stability later on\n    epsilon = 1e-6\n\n    for c in range(num_classes):\n        detections = []\n        ground_truths = []\n\n        # Go through all predictions and targets,\n        # and only add the ones that belong to the\n        # current class c\n        for detection in pred_boxes:\n            if detection[1] == c:\n                detections.append(detection)\n\n        for true_box in true_boxes:\n            if true_box[1] == c:\n                ground_truths.append(true_box)\n\n        # find the amount of bboxes for each training example\n        # Counter here finds how many ground truth bboxes we get\n        # for each training example, so let's say img 0 has 3,\n        # img 1 has 5 then we will obtain a dictionary with:\n        # amount_bboxes = {0:3, 1:5}\n        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n\n        # We then go through each key, val in this dictionary\n        # and convert to the following (w.r.t same example):\n        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n        for key, val in amount_bboxes.items():\n            amount_bboxes[key] = torch.zeros(val)\n\n        # sort by box probabilities which is index 2\n        detections.sort(key=lambda x: x[2], reverse=True)\n        TP = torch.zeros((len(detections)))\n        FP = torch.zeros((len(detections)))\n        total_true_bboxes = len(ground_truths)\n        \n        # If none exists for this class then we can safely skip\n        if total_true_bboxes == 0:\n            continue\n\n        for detection_idx, detection in enumerate(detections):\n            # Only take out the ground_truths that have the same\n            # training idx as detection\n            ground_truth_img = [\n                bbox for bbox in ground_truths if bbox[0] == detection[0]\n            ]\n\n            num_gts = len(ground_truth_img)\n            best_iou = 0\n\n            for idx, gt in enumerate(ground_truth_img):\n                iou_temp = iou(\n                    torch.tensor(detection[3:]),\n                    torch.tensor(gt[3:]),\n                    box_format=box_format,\n                )\n\n                if iou_temp > best_iou:\n                    best_iou = iou_temp\n                    best_gt_idx = idx\n\n            if best_iou > iou_threshold:\n                # only detect ground truth detection once\n                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n                    # true positive and add this bounding box to seen\n                    TP[detection_idx] = 1\n                    amount_bboxes[detection[0]][best_gt_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n\n            # if IOU is lower then the detection is a false positive\n            else:\n                FP[detection_idx] = 1\n\n        TP_cumsum = torch.cumsum(TP, dim=0)\n        FP_cumsum = torch.cumsum(FP, dim=0)\n        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n        # torch.trapz for numerical integration\n        average_precisions.append(torch.trapz(precisions, recalls))\n\n    return sum(average_precisions) / len(average_precisions)\n\n\ndef get_bboxes(\n    loader,\n    model,\n    iou_threshold,\n    threshold,\n    pred_format=\"cells\",\n    box_format=\"midpoint\",\n    device=\"cuda\",\n):\n    all_pred_boxes = []\n    all_true_boxes = []\n\n    # make sure model is in eval before get bboxes\n    model.eval()\n    train_idx = 0\n\n    for batch_idx, (x, labels) in enumerate(loader):\n        x = x.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size = x.shape[0]\n        true_bboxes = cellboxes_to_boxes(labels)\n        bboxes = cellboxes_to_boxes(predictions)\n\n        for idx in range(batch_size):\n            nms_boxes = nms(\n                bboxes[idx],\n                iou_threshold=iou_threshold,\n                threshold=threshold,\n                box_format=box_format,\n            )\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_bboxes[idx]:\n                # many will get converted to 0 pred\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            train_idx += 1\n\n    model.train()\n    return all_pred_boxes, all_true_boxes\n\n\ndef convert_cellboxes(predictions, S=7):\n\n    predictions = predictions.to(\"cpu\")\n    batch_size = predictions.shape[0]\n    predictions = predictions.reshape(batch_size, 7, 7, 30)\n    bboxes1 = predictions[..., 21:25]\n    bboxes2 = predictions[..., 26:30]\n    scores = torch.cat((predictions[..., 20].unsqueeze(0), predictions[..., 25].unsqueeze(0)), dim=0)\n    \n    best_box = scores.argmax(0).unsqueeze(-1)\n    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n    w_y = 1 / S * best_boxes[..., 2:4]\n    \n    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n    predicted_class = predictions[..., :20].argmax(-1).unsqueeze(-1)\n    best_confidence = torch.max(predictions[..., 20], predictions[..., 25]).unsqueeze(-1)\n    converted_preds = torch.cat((predicted_class, best_confidence, converted_bboxes), dim=-1)\n\n    return converted_preds\n\n\ndef cellboxes_to_boxes(out, S=7):\n    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n    converted_pred[..., 0] = converted_pred[..., 0].long()\n    all_bboxes = []\n\n    for ex_idx in range(out.shape[0]):\n        bboxes = []\n\n        for bbox_idx in range(S * S):\n            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n        all_bboxes.append(bboxes)\n\n    return all_bboxes\n","metadata":{"execution":{"iopub.status.busy":"2021-08-30T16:43:49.248134Z","iopub.execute_input":"2021-08-30T16:43:49.248938Z","iopub.status.idle":"2021-08-30T16:43:49.288303Z","shell.execute_reply.started":"2021-08-30T16:43:49.24888Z","shell.execute_reply":"2021-08-30T16:43:49.287534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## LOSS\nimport torch\nimport torch.nn as nn\nfrom torch import sqrt\n\nclass current_loss(nn.Module):\n    def __init__(self, S, B, C, box_type):\n        super(current_loss, self).__init__()\n        self.S     = S\n        self.B     = B\n        self.C     = C\n        self.btype = box_type\n        self.mse   = nn.MSELoss(reduction=\"sum\")\n        \n        self.lambda_noobj = 0.5\n        self.lambda_coord = 5\n        \n    def forward(self, predictions, target):\n\n        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n\n        # Calculate IoU for the two predicted bounding boxes\n        iou_b1 = iou(predictions[..., 21:25], target[..., 21:25], \"midpoint\")\n        iou_b2 = iou(predictions[..., 26:30], target[..., 21:25], \"midpoint\")\n        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n\n        # Take the box with highest IoU out of the two prediction\n        iou_maxes, bestbox = torch.max(ious, dim=0)\n        exists_box = target[..., 20].unsqueeze(3)  # in paper this is Iobj_i\n\n        # Loss for box coordinates and dimensions\n\n        box_predictions = exists_box * (\n            (\n                bestbox * predictions[..., 26:30]\n                + (1 - bestbox) * predictions[..., 21:25]\n            )\n        )\n\n        box_targets = exists_box * target[..., 21:25]\n\n        # Take sqrt of width, height of boxes to ensure that\n        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n            torch.abs(box_predictions[..., 2:4] + 1e-6)\n        )\n        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n\n        box_loss = self.mse(\n            torch.flatten(box_predictions, end_dim=-2),\n            torch.flatten(box_targets, end_dim=-2),\n        )\n\n        # Loss for existance of object\n\n        # pred_box is the confidence score for the bbox with highest IoU\n        pred_box = (\n            bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21]\n        )\n\n        object_loss = self.mse(\n            torch.flatten(exists_box * pred_box),\n            torch.flatten(exists_box * target[..., 20:21]),\n        )\n\n        # Loss for non existance of object\n\n        no_object_loss = self.mse(\n            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n        )\n\n        no_object_loss += self.mse(\n            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1)\n        )\n\n        #Class loss\n        \n        class_loss = self.mse(\n            torch.flatten(exists_box * predictions[..., :20], end_dim=-2,),\n            torch.flatten(exists_box * target[..., :20], end_dim=-2,),\n        )\n\n        loss = (\n            self.lambda_coord * box_loss  # first two rows in paper\n            + object_loss  # third row in paper\n            + self.lambda_noobj * no_object_loss  # forth row\n            + class_loss  # fifth row\n        )\n\n        return loss","metadata":{"execution":{"iopub.status.busy":"2021-08-30T16:43:49.28958Z","iopub.execute_input":"2021-08-30T16:43:49.289979Z","iopub.status.idle":"2021-08-30T16:43:49.308785Z","shell.execute_reply.started":"2021-08-30T16:43:49.289944Z","shell.execute_reply":"2021-08-30T16:43:49.308015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## MAIN\nimport torch\nimport pandas\nimport matplotlib.pyplot as plt\nimport os\n# import import_ipynb\n\nos.getcwd()\n\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as T\n\nS = 7\nB = 2\nC = 20\n\n# csv_directory, images_directory, labels_directory, transformation\ncsv_dir  = '../input/pascalvoc-yolo/train.csv'\nimg_dir  = '../input/pascalvoc-yolo/images'\nlbl_dir  = '../input/pascalvoc-yolo/labels'\n\nto = [T.ToTensor(),T.Resize((448,448))]\ndataset  = load_ds(csv_dir, img_dir, lbl_dir,\n                   transform=to, S=S, B=B, C=C)\n\nfrom torch.utils.data.dataloader import DataLoader\n\nbatch_size  = 64\nnum_workers = 4\ntrain_dl    = DataLoader(dataset, batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n\nS = 7\nB = 2\nC = 20\n\n# device = xm.xla_device()\ndevice = 'cuda' if torch.cuda.is_available else 'cpu'\n# model  = architecture(S=S, B=B, C=C).to(device)\n# model  = torch.load('../input/mymodel/mymodel')\nmodel  = torch.load('mymodel')\n\nfrom tqdm import tqdm\nimport torch.optim as optim\nimport warnings\nwarnings.filterwarnings('ignore')\n\nmyloss    = current_loss(S, B, C, 'midpoint')\noptimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=0)\n# device    = 'cuda' if torch.cuda.is_available() else 'cpu'\n","metadata":{"execution":{"iopub.status.busy":"2021-08-30T16:43:49.310106Z","iopub.execute_input":"2021-08-30T16:43:49.31048Z","iopub.status.idle":"2021-08-30T16:43:49.561936Z","shell.execute_reply.started":"2021-08-30T16:43:49.310413Z","shell.execute_reply":"2021-08-30T16:43:49.559883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Using :\", device)\nepochs = 10\nloss_trend = []\nfor epoch in range(epochs):\n    avg_loss  = []\n    loop = tqdm(enumerate(train_dl), total=len(train_dl), leave=False)\n    for batch_idx, (x, y) in loop:\n        x = x.to(device)\n        y = y.to(device)\n        loss = myloss(model(x), y)\n        avg_loss.append(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        loop.set_description(f\"Epoch: [{epoch+1}/{epochs}]\")\n        loop.set_postfix(loss=loss.item())\n        \n    loss_trend.append(sum(avg_loss)/len(avg_loss))    \n    pred_boxes, target_boxes = get_bboxes(train_dl, model, iou_threshold=0.5, threshold=0.4)\n    mAP = get_map(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n    print(f\"Epoch : {epoch+1}\", f\" LOSS : {loss_trend[-1]}\", f\" Train mAP : {mAP}\")\n    \ntorch.save(model, 'mymodel')\nprint(\"Model saved!\")\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-30T16:43:49.562801Z","iopub.status.idle":"2021-08-30T16:43:49.563194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}