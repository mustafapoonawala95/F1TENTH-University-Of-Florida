{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n# !pip install -U numpy\n# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version 1.9 --apt-packages libomp5 libopenblas-dev\n","metadata":{"execution":{"iopub.status.busy":"2021-09-09T20:00:16.720997Z","iopub.execute_input":"2021-09-09T20:00:16.721511Z","iopub.status.idle":"2021-09-09T20:00:16.725057Z","shell.execute_reply.started":"2021-09-09T20:00:16.721430Z","shell.execute_reply":"2021-09-09T20:00:16.724287Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# import torch\n# !export XLA_USE_BF16=1\n# import torch_xla.core.xla_model as xm\n# print(xm.xla_device())","metadata":{"execution":{"iopub.status.busy":"2021-09-09T20:00:16.729896Z","iopub.execute_input":"2021-09-09T20:00:16.730408Z","iopub.status.idle":"2021-09-09T20:00:16.738643Z","shell.execute_reply.started":"2021-09-09T20:00:16.730344Z","shell.execute_reply":"2021-09-09T20:00:16.737770Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def prepare_dataset():\n\n    # PRE-PROCESS LISA TRAFFIC LIGHT DATASET\n    import glob\n    import os\n    import shutil\n    import csv\n\n    # make directories for dataset\n    try:\n        os.mkdir('./dataset')\n        print(\"Directory 'dataset' is created\")\n    except OSError:\n        print(\"Directory 'dataset' already exists\")\n\n    try:\n        os.mkdir('./dataset/images')\n        print(\"Directory 'images' is created\")\n    except OSError:\n        print(\"Directory 'images' already exists\")\n\n    try:\n        os.mkdir('./dataset/labels')\n        print(\"Directory 'labels' is created\")\n    except OSError:\n        print(\"Directory 'labels' already exists\")\n\n    # # Copying all images to one folder\n    train_path = ['../input/lisa-traffic-light-dataset/dayTrain/dayTrain/dayClip*',\n                 '../input/lisa-traffic-light-dataset/nightTrain/nightTrain/nightClip*']\n    for i in train_path:\n        print(i)\n        for a in glob.glob(i):\n            for b in glob.glob(a+'/*'):\n                for c in glob.glob(b+'/*'):\n                    shutil.copy(c, os.getcwd()+'/dataset/images/')\n\n    for i in glob.glob('../input/lisa-traffic-light-dataset/*Sequence*'):\n        print(i)\n        for j in glob.glob(i+'/*'):\n            for k in glob.glob(j+'/*'):\n                for l in glob.glob(k+'/*'):\n                    shutil.copy(l, os.getcwd()+'/dataset/images/')\n\n    # Copying label files to labels folder and writing to one file\n    with open('./dataset/trainLabels.csv', 'w', newline=\"\") as f:\n        for i in glob.glob('../input/lisa-traffic-light-dataset/Annotations/Annotations/*Train'):\n            for j in glob.glob(i+'/*'):\n                for k in glob.glob(j+'/*BOX.csv'):\n                    file = open(k)\n                    lines = file.readlines()[1:]    \n                    for line in lines:\n                        writer = csv.writer(f)\n                        # write the data\n                        writer.writerow([line[:-1]])\n\n    with open('./dataset/testLabels.csv', 'w', newline=\"\") as f:\n        for i in glob.glob('../input/lisa-traffic-light-dataset/Annotations/Annotations/*Sequence*'):\n            for j in glob.glob(i+'/*BOX.csv'):\n                file = open(j)\n                lines = file.readlines()[1:]    \n                for line in lines:\n                    writer = csv.writer(f)\n                    # write the data\n                    writer.writerow([line[:-1]])\n\n    ## CREATING A LABEL MAP FOR CLASSES\n    classes = {}\n    file = open('./dataset/trainLabels.csv')\n    for line in file.readlines():\n        temp = line.split(';')\n        try:\n            classes[temp[1]] += 1\n        except KeyError:\n            classes[temp[1]] = 1\n    file.close()\n\n    file = open('./dataset/testLabels.csv')\n    for line in file.readlines():\n        temp = line.split(';')\n        try:\n            classes[temp[1]] += 1\n        except KeyError:\n            classes[temp[1]] = 1\n    file.close()\n\n    counter = 0\n    label_map = {}\n    for i in classes:\n        label_map[i] = counter\n        counter += 1\n\n    label_map['goForward'] = 0\n    print(label_map)\n\n    # WRITING ALL DATA TO RIGHT FORMAT\n    def get_line(line):\n        temp = line.split(\";\")\n        image_name = temp[0].split(\"/\")[1]\n\n        # Create the data line\n        class_label = str(label_map[temp[1]])\n        x = str( (int(temp[2]) + int(temp[4]))*0.5 / 1280)\n        y = str( (int(temp[3]) + int(temp[5]))*0.5 / 960)\n        w = str( (int(temp[4]) - int(temp[2])) /1280)\n        h = str( (int(temp[5]) - int(temp[3])) /960)\n\n        data = class_label + ' '\n        data += x + ' '\n        data += y + ' '\n        data += w + ' '\n        data += h\n\n        return image_name, data    \n\n\n    # ------------- FOR TRAIN DATA -------------------#\n\n    file = open('./dataset/trainLabels.csv')\n    annotations = {}\n    for line in file.readlines():\n        image_name, data = get_line(line)\n        try:\n            annotations[image_name]\n            annotations[image_name].append(data)\n        except KeyError:\n            annotations[image_name] = [data]\n    file.close()\n\n\n    with open('./dataset/train.csv', 'w', newline=\"\") as f1:\n\n        for i in annotations:\n            # Creating images and its text file associations\n            text_file = i.split('.')[0] + \".txt\"\n            string = str(i) + str(\",\")+ str(text_file)\n            writer1 = csv.writer(f1, quoting=csv.QUOTE_NONE, escapechar='', delimiter=\" \")\n            writer1.writerow([string])\n\n            # Creating text files containing class and bounding boxes\n            with open('./dataset/labels/'+text_file, 'w', newline=\"\") as f2:\n                for text_data in annotations[i]:\n                    writer2 = csv.writer(f2)\n                    writer2.writerow([text_data])\n\n\n    # ------------- FOR TEST DATA -------------------#\n\n    file = open('./dataset/testLabels.csv')\n    annotations = {}\n    for line in file.readlines():\n        image_name, data = get_line(line)\n        try:\n            annotations[image_name]\n            annotations[image_name].append(data)\n        except KeyError:\n            annotations[image_name] = [data]\n    file.close()\n\n\n    with open('./dataset/test.csv', 'w', newline=\"\") as f1:\n\n        for i in annotations:\n            # Creating images and its text file associations\n            text_file = i.split('.')[0] + \".txt\"\n            string = str(i) + str(\",\")+ str(text_file)\n            writer1 = csv.writer(f1, quoting=csv.QUOTE_NONE, escapechar='', delimiter=\" \")\n            writer1.writerow([string])\n\n            # Creating text files containing class and bounding boxes\n            with open('./dataset/labels/'+text_file, 'w', newline=\"\") as f2:\n                for text_data in annotations[i]:\n                    writer2 = csv.writer(f2)\n                    writer2.writerow([text_data])","metadata":{"execution":{"iopub.status.busy":"2021-09-09T20:00:16.740618Z","iopub.execute_input":"2021-09-09T20:00:16.740964Z","iopub.status.idle":"2021-09-09T20:00:16.847681Z","shell.execute_reply.started":"2021-09-09T20:00:16.740930Z","shell.execute_reply":"2021-09-09T20:00:16.846543Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"## Architecture\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass architecture(nn.Module):\n    def __init__(self, S, B, C):\n        # S= No.of boxes, B=No.of prediction boxes, C=No.of classes\n        super(architecture, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3,64,kernel_size=7,stride=2,padding=3),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.1),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(64,192,kernel_size=3,stride=1,padding=1),\n            nn.BatchNorm2d(192),\n            nn.LeakyReLU(0.1),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(192,128,kernel_size=1,stride=1,padding=0),\n            nn.Conv2d(128,256,kernel_size=3,stride=1,padding=1),\n            nn.Conv2d(256,256,kernel_size=1,stride=1,padding=0),\n            nn.Conv2d(256,512,kernel_size=3,stride=1,padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.1),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(512,256,kernel_size=1,stride=1,padding=0),\n            nn.Conv2d(256,512,kernel_size=3,stride=1,padding=1),\n            nn.Conv2d(512,256,kernel_size=1,stride=1,padding=0),\n            nn.Conv2d(256,512,kernel_size=3,stride=1,padding=1),\n            nn.Conv2d(512,256,kernel_size=1,stride=1,padding=0),\n            nn.Conv2d(256,512,kernel_size=3,stride=1,padding=1),\n            nn.Conv2d(512,256,kernel_size=1,stride=1,padding=0),\n            nn.Conv2d(256,512,kernel_size=3,stride=1,padding=1),\n            nn.Conv2d(512,512,kernel_size=1,stride=1,padding=0),\n            nn.Conv2d(512,1024,kernel_size=3,stride=1,padding=1),\n            nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.1),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=0),\n            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=0),\n            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(1024, 1024, kernel_size=3, stride=2, padding=1),\n            nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1),\n            \n            nn.Flatten(),\n            nn.Linear(50176, 4096),\n            nn.Dropout(0.0),\n            nn.LeakyReLU(0.1),\n            nn.Linear(4096, S * S * (C + B * 5))\n        )\n        \n    def forward(self, x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T20:00:16.850928Z","iopub.execute_input":"2021-09-09T20:00:16.851425Z","iopub.status.idle":"2021-09-09T20:00:17.202667Z","shell.execute_reply.started":"2021-09-09T20:00:16.851393Z","shell.execute_reply":"2021-09-09T20:00:17.201825Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"## getDataset\nfrom PIL import Image\nimport torch\nimport pandas as pd\nimport os\n\nclass load_ds(torch.utils.data.Dataset):\n    def __init__(self, csv, images_dir, label_dir, S, B, C, transform=None):\n        self.associations = pd.read_csv(csv)\n        self.img_dir      = images_dir\n        self.lbl_dir      = label_dir\n        self.transform    = transform\n        self.S            = S\n        self.B            = B\n        self.C            = C\n        \n    def __len__(self):\n        return len(self.associations)\n        \n    def __getitem__(self, index):\n        img_path   = os.path.join(self.img_dir, self.associations.iloc[index, 0])\n        label_path = os.path.join(self.lbl_dir, self.associations.iloc[index, 1])\n        bboxes     = []\n        \n        with open(label_path) as x:\n            for i in x.readlines():\n                class_label, x, y, w, h = [float(x) if float(x) != int(float(x)) else int(x)\n                                            for x in i.replace(\"\\n\", \"\").split()]\n                \n                bboxes.append([class_label, x, y, w, h])\n                \n#         print(bboxes)\n        \n        img = Image.open(img_path)\n        if self.transform:\n            for i in self.transform:\n                img, bboxes = i(img), bboxes\n                \n        \n        labels = torch.zeros((self.S, self.S, self.C + self.B*5))\n        for box in bboxes:\n            class_label, x, y, w, h = box\n            class_label = int(class_label)\n\n            # i,j represents the cell row and cell column\n            i, j = int(self.S * y), int(self.S * x)\n            x_cell, y_cell = self.S * x - j, self.S * y - i\n            w_cell, h_cell = (w*self.S, h*self.S)\n            \n            \n            if labels[i, j, self.C] == 0:\n                # Set 1 if there exists an object\n                labels[i, j, self.C] = 1\n\n                # Box coordinates\n                box_coords = torch.tensor([x_cell, y_cell, w_cell, h_cell])\n                labels[i, j, self.C+1:self.C+5] = box_coords\n\n                # one hot encoding for class label\n                labels[i, j, class_label] = 1\n\n        return img, labels","metadata":{"execution":{"iopub.status.busy":"2021-09-09T20:00:17.203990Z","iopub.execute_input":"2021-09-09T20:00:17.204342Z","iopub.status.idle":"2021-09-09T20:00:17.220172Z","shell.execute_reply.started":"2021-09-09T20:00:17.204299Z","shell.execute_reply":"2021-09-09T20:00:17.219397Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom collections import Counter\n\ndef iou(x, y, box_format):\n\n        if box_format == \"box\":\n            box1_x1 = x[..., 0]\n            box1_y1 = x[..., 1]\n            box1_x2 = x[..., 2]\n            box1_y2 = x[..., 3]\n            box2_x1 = y[..., 0]\n            box2_y1 = y[..., 1]\n            box2_x1 = y[..., 2]\n            box2_y1 = y[..., 3]\n        elif box_format == \"midpoint\":\n            box1_x1 = x[..., 0:1] - x[..., 2:3] / 2\n            box1_y1 = x[..., 1:2] - x[..., 3:4] / 2\n            box1_x2 = x[..., 0:1] + x[..., 2:3] / 2\n            box1_y2 = x[..., 1:2] + x[..., 3:4] / 2\n            box2_x1 = y[..., 0:1] - y[..., 2:3] / 2\n            box2_y1 = y[..., 1:2] - y[..., 3:4] / 2\n            box2_x2 = y[..., 0:1] + y[..., 2:3] / 2\n            box2_y2 = y[..., 1:2] + y[..., 3:4] / 2\n\n\n        x1 = torch.max(box1_x1, box2_x1)\n        y1 = torch.max(box1_y1, box2_y1)\n        x2 = torch.min(box1_x2, box2_x2)\n        y2 = torch.min(box1_y2, box2_y2)\n\n\n        # .clamp(0) is for the case when they do not intersect\n        intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n\n        box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n        box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n\n        return intersection / (box1_area + box2_area - intersection + 1e-6)\n    \n\ndef get_map(pred_boxes, true_boxes, iou_threshold, box_format, num_classes):\n\n    # list storing all AP for respective classes\n    average_precisions = []\n\n    # used for numerical stability later on\n    epsilon = 1e-6\n\n    for c in range(num_classes):\n        detections = []\n        ground_truths = []\n\n        # Go through all predictions and targets,\n        # and only add the ones that belong to the\n        # current class c\n        for detection in pred_boxes:\n            if detection[1] == c:\n                detections.append(detection)\n\n        for true_box in true_boxes:\n            if true_box[1] == c:\n                ground_truths.append(true_box)\n\n        # find the amount of bboxes for each training example\n        # Counter here finds how many ground truth bboxes we get\n        # for each training example, so let's say img 0 has 3,\n        # img 1 has 5 then we will obtain a dictionary with:\n        # amount_bboxes = {0:3, 1:5}\n        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n\n        # We then go through each key, val in this dictionary\n        # and convert to the following (w.r.t same example):\n        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n        for key, val in amount_bboxes.items():\n            amount_bboxes[key] = torch.zeros(val)\n\n        # sort by box probabilities which is index 2\n        detections.sort(key=lambda x: x[2], reverse=True)\n        TP = torch.zeros((len(detections)))\n        FP = torch.zeros((len(detections)))\n        total_true_bboxes = len(ground_truths)\n        \n        # If none exists for this class then we can safely skip\n        if total_true_bboxes == 0:\n            continue\n\n        for detection_idx, detection in enumerate(detections):\n            # Only take out the ground_truths that have the same\n            # training idx as detection\n            ground_truth_img = [\n                bbox for bbox in ground_truths if bbox[0] == detection[0]\n            ]\n\n            num_gts = len(ground_truth_img)\n            best_iou = 0\n\n            for idx, gt in enumerate(ground_truth_img):\n                iou_temp = iou(\n                    torch.tensor(detection[3:]),\n                    torch.tensor(gt[3:]),\n                    box_format=box_format,\n                )\n\n                if iou_temp > best_iou:\n                    best_iou = iou_temp\n                    best_gt_idx = idx\n\n            if best_iou > iou_threshold:\n                # only detect ground truth detection once\n                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n                    # true positive and add this bounding box to seen\n                    TP[detection_idx] = 1\n                    amount_bboxes[detection[0]][best_gt_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n\n            # if IOU is lower then the detection is a false positive\n            else:\n                FP[detection_idx] = 1\n\n        TP_cumsum = torch.cumsum(TP, dim=0)\n        FP_cumsum = torch.cumsum(FP, dim=0)\n        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n        \n        # torch.trapz for numerical integration\n        average_precisions.append(torch.trapz(precisions, recalls))\n\n    return sum(average_precisions) / len(average_precisions)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T20:00:17.221594Z","iopub.execute_input":"2021-09-09T20:00:17.221970Z","iopub.status.idle":"2021-09-09T20:00:17.245067Z","shell.execute_reply.started":"2021-09-09T20:00:17.221934Z","shell.execute_reply":"2021-09-09T20:00:17.244088Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_bboxes(loader, model, iou_threshold, threshold,\n               device, S, C, pred_format=\"cells\", box_format=\"midpoint\"):\n    \n    all_pred_boxes = []\n    all_true_boxes = []\n\n    # make sure model is in eval before get bboxes\n    model.eval()\n    train_idx = 0\n\n    for batch_idx, (x, labels) in enumerate(loader):\n        \n#         print(f\"get_bboxes batch idx: {batch_idx}\")\n        \n        x      = x.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size  = x.shape[0]\n        true_bboxes = cellboxes_to_boxes(labels, S, C)\n        pred_bboxes = cellboxes_to_boxes(predictions, S, C)\n        \n#         print(f\"true_bboxes length: {len(true_bboxes)}\", f\"pred_bboxes length: {len(pred_bboxes)}\" )\n\n        for idx in range(batch_size):\n            nms_boxes = nms(\n                pred_bboxes[idx],\n                iou_threshold=iou_threshold,\n                threshold=threshold,\n                box_format=box_format)\n            \n#             print(f\"Length of nms_boxes: {len(nms_boxes)}\" )\n            \n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_bboxes[idx]:\n                # many will get converted to 0 pred\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            train_idx += 1\n        \n        break\n        \n    model.train()\n    \n    return all_pred_boxes, all_true_boxes\n\n\ndef nms(bboxes, iou_threshold, threshold, box_format=\"box\"):\n\n    assert type(bboxes) == list\n\n    bboxes = [box for box in bboxes if box[1] > threshold]\n    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n    bboxes_after_nms = []\n    \n#     print(f\"Length of nms bboxes: {len(bboxes)}\" )\n    \n    while bboxes:\n        chosen_box = bboxes.pop(0)\n        \n        bboxes = [box for box in bboxes if box[0] != chosen_box[0]\n                or iou(torch.tensor(chosen_box[2:]), torch.tensor(box[2:]), box_format=box_format) < iou_threshold]\n\n        bboxes_after_nms.append(chosen_box)\n        \n    return bboxes_after_nms\n\n\ndef cellboxes_to_boxes(out, S, C):\n    converted_pred = convert_cellboxes(out, S, C).reshape(out.shape[0], S * S, -1)\n    converted_pred[..., 0] = converted_pred[..., 0].long()\n    all_bboxes = []\n\n    for ex_idx in range(out.shape[0]):\n        bboxes = []\n\n        for bbox_idx in range(S * S):\n            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n        all_bboxes.append(bboxes)\n\n    return all_bboxes\n\n\ndef convert_cellboxes(predictions, S, C):\n\n    predictions = predictions.to(\"cpu\")\n    batch_size  = predictions.shape[0]\n    predictions = predictions.reshape(batch_size, S, S, C + B*5)\n    bboxes1     = predictions[..., C+1:C+5]\n    bboxes2     = predictions[..., C+6:C+10]\n    scores      = torch.cat((predictions[..., C].unsqueeze(0), predictions[..., C+5].unsqueeze(0)), dim=0)\n    \n    best_box     = scores.argmax(0).unsqueeze(-1)\n    best_boxes   = bboxes1 * (1 - best_box) + best_box * bboxes2\n    cell_indices = torch.arange(S).repeat(batch_size, S, 1).unsqueeze(-1)\n    \n    x   = 1 / S * (best_boxes[..., :1] + cell_indices)\n    y   = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n    w_y = 1 / S * best_boxes[..., 2:4]\n    \n    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n    predicted_class  = predictions[..., :C].argmax(-1).unsqueeze(-1)\n    best_confidence  = torch.max(predictions[..., C], predictions[..., C+5]).unsqueeze(-1)\n    converted_preds  = torch.cat((predicted_class, best_confidence, converted_bboxes), dim=-1)\n    \n    return converted_preds","metadata":{"execution":{"iopub.status.busy":"2021-09-09T20:00:17.246646Z","iopub.execute_input":"2021-09-09T20:00:17.247042Z","iopub.status.idle":"2021-09-09T20:00:17.269021Z","shell.execute_reply.started":"2021-09-09T20:00:17.247006Z","shell.execute_reply":"2021-09-09T20:00:17.268233Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"## LOSS\nimport torch\nimport torch.nn as nn\nfrom torch import sqrt\n\nclass current_loss(nn.Module):\n    def __init__(self, S, B, C, box_type):\n        super(current_loss, self).__init__()\n        self.S     = S\n        self.B     = B\n        self.C     = C\n        self.btype = box_type\n        self.mse   = nn.MSELoss(reduction=\"sum\")\n        \n        self.lambda_noobj = 0.5\n        self.lambda_coord = 5\n        \n    def forward(self, predictions, target):\n\n        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n\n        # Calculate IoU for the two predicted bounding boxes\n        iou_b1 = iou(predictions[..., self.C+1:self.C+5], target[..., self.C+1:self.C+5], \"midpoint\")\n        iou_b2 = iou(predictions[..., self.C+6:self.C+10], target[..., self.C+1:self.C+5], \"midpoint\")\n        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n\n        # Take the box with highest IoU out of the two prediction\n        iou_maxes, bestbox = torch.max(ious, dim=0)\n        exists_box = target[..., self.C+1].unsqueeze(3)  # in paper this is Iobj_i\n\n        # Loss for box coordinates and dimensions\n\n        box_predictions = exists_box * (\n            (\n                bestbox * predictions[..., self.C+6:self.C+10]\n                + (1 - bestbox) * predictions[..., self.C+1:self.C+5]\n            )\n        )\n\n        box_targets = exists_box * target[..., self.C+1:self.C+5]\n\n        # Take sqrt of width, height of boxes to ensure that\n        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n            torch.abs(box_predictions[..., 2:4] + 1e-6)\n        )\n        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n\n        box_loss = self.mse(\n            torch.flatten(box_predictions, end_dim=-2),\n            torch.flatten(box_targets, end_dim=-2),\n        )\n\n        # Loss for existance of object\n\n        # pred_box is the confidence score for the bbox with highest IoU\n        pred_box = (\n            bestbox * predictions[..., self.C+5:self.C+6] + (1 - bestbox) * predictions[..., self.C:self.C+1]\n        )\n\n        object_loss = self.mse(\n            torch.flatten(exists_box * pred_box),\n            torch.flatten(exists_box * target[..., self.C:self.C+1]),\n        )\n\n        # Loss for non existance of object\n\n        no_object_loss = self.mse(\n            torch.flatten((1 - exists_box) * predictions[..., self.C:self.C+1], start_dim=1),\n            torch.flatten((1 - exists_box) * target[..., self.C:self.C+1], start_dim=1),\n        )\n\n        no_object_loss += self.mse(\n            torch.flatten((1 - exists_box) * predictions[..., self.C+5:self.C+6], start_dim=1),\n            torch.flatten((1 - exists_box) * target[..., self.C:self.C+1], start_dim=1)\n        )\n\n        #Class loss\n        \n        class_loss = self.mse(\n            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2,),\n            torch.flatten(exists_box * target[..., :self.C], end_dim=-2,),\n        )\n\n        loss = (\n            self.lambda_coord * box_loss  # first two rows in paper\n            + object_loss  # third row in paper\n            + self.lambda_noobj * no_object_loss  # forth row\n            + class_loss  # fifth row\n        )\n\n        return loss","metadata":{"execution":{"iopub.status.busy":"2021-09-09T20:00:17.271636Z","iopub.execute_input":"2021-09-09T20:00:17.272073Z","iopub.status.idle":"2021-09-09T20:00:17.292370Z","shell.execute_reply.started":"2021-09-09T20:00:17.272032Z","shell.execute_reply":"2021-09-09T20:00:17.291568Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"## MAIN\nimport torch\nimport pandas\nimport matplotlib.pyplot as plt\nimport os\n# import import_ipynb\n\nos.getcwd()\n\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as T\n\nS = 16\nB = 2\nC = 6\n\n# prepare_dataset()\n\n# csv_directory, images_directory, labels_directory, transformation\ntrain_dir  = './dataset/train.csv'\nvalid_dir  = './dataset/test.csv'\nimg_dir    = './dataset/images'\nlbl_dir    = './dataset/labels'\n\nto = [T.ToTensor(),T.Resize((448,448))]\ntrain_ds  = load_ds(train_dir, img_dir, lbl_dir,\n                   transform=to, S=S, B=B, C=C)\nvalid_ds  = load_ds(valid_dir, img_dir, lbl_dir,\n                   transform=to, S=S, B=B, C=C)\n\nfrom torch.utils.data.dataloader import DataLoader\n\nbatch_size  = 64\nnum_workers = 4\ntrain_dl    = DataLoader(train_ds, batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\nvalid_dl    = DataLoader(valid_ds, batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n\n# device = xm.xla_device()\ndevice = 'cuda' if torch.cuda.is_available else 'cpu'\nmodel  = architecture(S=S, B=B, C=C).to(device)\n# model  = torch.load('../input/mymodel1/mymodel')\n# model  = torch.load('mymodel')\n\nfrom tqdm import tqdm\nimport torch.optim as optim\nimport warnings\nwarnings.filterwarnings('ignore')\n\nmyloss    = current_loss(S, B, C, 'midpoint')\noptimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T20:00:17.295698Z","iopub.execute_input":"2021-09-09T20:00:17.295941Z","iopub.status.idle":"2021-09-09T20:00:21.597650Z","shell.execute_reply.started":"2021-09-09T20:00:17.295919Z","shell.execute_reply":"2021-09-09T20:00:21.596779Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# for idx, (x,y) in enumerate(train_dl):\n#     x = x.to(device)\n#     preds = model(x)\n#     print(idx, x.shape, y.shape, preds.shape)\n#     print(model.model)\n#     break","metadata":{"execution":{"iopub.status.busy":"2021-09-09T20:00:21.598865Z","iopub.execute_input":"2021-09-09T20:00:21.599212Z","iopub.status.idle":"2021-09-09T20:00:21.603875Z","shell.execute_reply.started":"2021-09-09T20:00:21.599177Z","shell.execute_reply":"2021-09-09T20:00:21.602650Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"epochs     = 10\nloss_trend = []\nprint(\"Using :\", device)\n\nfor epoch in range(epochs):\n    avg_loss  = []\n    loop = tqdm(enumerate(train_dl), total=len(train_dl), leave=False)\n    for batch_idx, (x, y) in loop:\n        x = x.to(device)\n        y = y.to(device)\n        loss = myloss(model(x), y)\n        avg_loss.append(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        loop.set_description(f\"Epoch: [{epoch+1}/{epochs}]\")\n        loop.set_postfix(loss=loss.item())\n        \n    loss_trend.append(sum(avg_loss)/len(avg_loss))\n    \n    pred_boxes, target_boxes = get_bboxes(valid_dl, model, iou_threshold=0.5, threshold=0.4, device=device, S=S, C=C)\n    mAP = None\n    try:\n        mAP = get_map(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=C)\n    except ZeroDivisionError:\n        print(\"ZeroDivisionError\")\n        \n    print(f\"Epoch : {epoch+1}\", f\" Training Loss : {loss_trend[-1]}\", f\" Validation mAP : {mAP}\")   \n\n\ntorch.save(model, 'mymodel')\nprint(\"Model saved!\")\n","metadata":{"execution":{"iopub.status.busy":"2021-09-09T20:00:21.605530Z","iopub.execute_input":"2021-09-09T20:00:21.605894Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Using : cuda\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch : 1  Training Loss : 160.8836212158203  Validation mAP : 0.0\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch : 2  Training Loss : 78.28256225585938  Validation mAP : 0.0\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch : 3  Training Loss : 60.010650634765625  Validation mAP : 0.0\n","output_type":"stream"},{"name":"stderr","text":"Epoch: [4/10]:  67%|██████▋   | 190/282 [05:51<03:22,  2.20s/it, loss=55.6]","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}